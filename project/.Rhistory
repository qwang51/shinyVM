patient_data <- data.frame(
gender = sample(c('M', 'F'), N, replace=TRUE),
color = sample(c('red', 'green', 'blue'), N, replace=TRUE),
size = sample(c('S', 'M', 'L'), N, replace=TRUE),
age = round(rnorm(N, mean=60, sd=15))
)
with(patient_data, plot(foobium ~ age))
fit1 <- lm(foobium ~ age, patient_data)
abline(fit1, col="blue")
summary(fit1)
with(patient_data, {
plot(foobium ~ age, col=gender, xlim=c(0, 110))
legend("bottom", legend=levels(gender), text.col=seq_along(levels(gender)), bty='n')
})
fit2 <- lm(foobium ~ age*gender, patient_data)
points(patient_data$age, predict(fit2), col='yellow', pch=4)
p <- coef(fit2)
abline(p['(Intercept)'], p['age'])
abline(p['(Intercept)'] + p['genderM'], p['age'] + p['age:genderM'], col="red")
abline(v=0, lty=2)
summary(fit2)
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
N
patient_data <- data.frame(
gender = sample(c('M', 'F'), N, replace=TRUE),
color = sample(c('red', 'green', 'blue'), N, replace=TRUE),
size = sample(c('S', 'M', 'L'), N, replace=TRUE),
age = round(rnorm(N, mean=60, sd=15))
)
with(patient_data, plot(foobium ~ age))
fit1 <- lm(foobium ~ age, patient_data)
abline(fit1, col="blue")
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
foobium
N <- 2000
patient_data <- data.frame(
gender = sample(c('M', 'F'), N, replace=TRUE),
color = sample(c('red', 'green', 'blue'), N, replace=TRUE),
size = sample(c('S', 'M', 'L'), N, replace=TRUE),
age = round(rnorm(N, mean=60, sd=15))
)
with(patient_data, plot(foobium ~ age))
fit1 <- lm(foobium ~ age, patient_data)
abline(fit1, col="blue")
summary(fit1)
with(patient_data, {
plot(foobium ~ age, col=gender, xlim=c(0, 110))
legend("bottom", legend=levels(gender), text.col=seq_along(levels(gender)), bty='n')
})
fit2 <- lm(foobium ~ age*gender, patient_data)
points(patient_data$age, predict(fit2), col='yellow', pch=4)
p <- coef(fit2)
abline(p['(Intercept)'], p['age'])
abline(p['(Intercept)'] + p['genderM'], p['age'] + p['age:genderM'], col="red")
abline(v=0, lty=2)
summary(fit2)
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
abline(fit1, col="blue")
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
N <- 2000
patient_data <- data.frame(
gender = sample(c('M', 'F'), N, replace=TRUE),
color = sample(c('red', 'green', 'blue'), N, replace=TRUE),
size = sample(c('S', 'M', 'L'), N, replace=TRUE),
age = round(rnorm(N, mean=60, sd=15))
)
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
with(patient_data, plot(foobium ~ age))
fit1 <- lm(foobium ~ age, patient_data)
abline(fit1, col="blue")
summary(fit1)
with(patient_data, {
plot(foobium ~ age, col=gender, xlim=c(0, 110))
legend("bottom", legend=levels(gender), text.col=seq_along(levels(gender)), bty='n')
})
fit2 <- lm(foobium ~ age*gender, patient_data)
points(patient_data$age, predict(fit2), col='yellow', pch=4)
p <- coef(fit2)
abline(p['(Intercept)'], p['age'])
abline(p['(Intercept)'] + p['genderM'], p['age'] + p['age:genderM'], col="red")
abline(v=0, lty=2)
summary(fit2)
patient_data <- transform(patient_data,
foobium = 7 + (age - 35) * ifelse(gender=='M', 0.05, -0.05) + rnorm(length(gender)))
p
abline(p['(Intercept)'] + p['genderM'], p['age'] + p['age:genderM'], col="red")
x1 <- c(1,2,3,4)
y1 <- x1 + 5
x2 <- x1 + 7
y2 <- x2 - 7
x <- c(x1,x2)
y <- c(y1,y2)
postscript("simpson.eps", paper="special", width=4.5, height=3)
par(las=1)
par(mar=c(3,3,0.5,0.5))
par(mgp=c(2,1,0))
plot(x,y, cex=2, pch=21,
col=rep(c("blue", "red"), each=4), bg=rep(c("lightblue", "pink"), each=4),
xlim=range(x)+c(-2,2), ylim=range(y)+c(-2,2))
abline(lm(y1 ~ x1), col="blue", lwd=2)
abline(lm(y2 ~ x2), col="red", lwd=2)
abline(lm(y  ~ x), lwd=2, lty=2)
dev.off()
plot(x,y, cex=2, pch=21,
col=rep(c("blue", "red"), each=4), bg=rep(c("lightblue", "pink"), each=4),
xlim=range(x)+c(-2,2), ylim=range(y)+c(-2,2))
abline(lm(y1 ~ x1), col="blue", lwd=2)
abline(lm(y2 ~ x2), col="red", lwd=2)
abline(lm(y  ~ x), lwd=2, lty=2)
diabetes <- read.arff('http://www.cs.usfca.edu/~pfrancislyon/uci-diabetes.arff')
library(foreign)
diabetes <- read.arff('http://www.cs.usfca.edu/~pfrancislyon/uci-diabetes.arff')
View(diabetes)
library(foreign)
library(ggplot2)
library(reshape2)
library(corrplot)
diabetes <- read.arff('http://www.cs.usfca.edu/~pfrancislyon/uci-diabetes.arff')
#1. Preprocess the data so you can ignore those zeros that were used for missing data
#   (Write R code to replace those zeros with "NA").
diabetes[diabetes==0] <- NA
# 2. Perform t-tests to compare the means of the 8 predictor variables for those women who have
#    diabetes vs those who do not.
t.test(preg ~ class, diabetes)
t.test(plas ~ class, diabetes)
t.test(pres ~ class, diabetes)
t.test(skin ~ class, diabetes)
t.test(insu ~ class, diabetes)
t.test(mass ~ class, diabetes)
t.test(pedi ~ class, diabetes)
t.test(age ~ class, diabetes)
# 3. Plot the density distributions of each variable with fill color determined by diabetes status.
g <- ggplot(melt(diabetes), aes(x = value))
g + facet_wrap(~variable, scales = 'free') +
geom_density(aes(fill = class), alpha = 0.4)
# 4. Check the variables for correlations.
cor(diabetes[-9], use = 'complete')
corrplot(cor(diabetes[-9], use = 'complete'), method='number')
# 5. Fit a regression model to predict diabetes as a function of the other variables.
#    For the significant coefficients, state what one unit increase in each predictor variable
#    will result in for diabetes. In your report discuss the summary.
fit1 <- glm(class ~ preg + plas + pres + skin + insu + mass + pedi + age, diabetes,
family = binomial())
summary(fit1)
# The intercept and plas - plasma glucose concentration are the most significant coificent values
# mass, diabetes pedigree function and age are less significant variables. Others are not significant
# For each unit increase in plas, the result will increase 3.62e-02 unit
# Unit increase in mass -> 7.615e-02 unit increase in diabetes result
# Unit increase in pedi -> 1.097e+00 unit increase in diabetes result
# Unit increase in age -> 4.075e-02 unit increase in diabetes result
# 6. Fit a regression model to predict diastolic blood pressure as a function of two to four of
#    the other predictor variables. For the significant coefficients,
#    state what one unit increase in each predictor variable will result in for diastolic
#    blood pressure. In your report discuss the summary.
fit2 <- lm(pres ~ preg + plas + mass + age, diabetes)
summary(fit2)
# Intercept, mass and age are the most significant coefficients, plas is less significant
# and pedi is the least significant one
# Unit increase in:
# mass > 0.43217 increase in pres
# age > 0.29723 increase in pres
# plas > 0.03420 increase in pres
# preg > 0.30643 increase in pres
View(diabetes)
library(foreign)
library(ggplot2)
library(reshape2)
library(corrplot)
diabetes <- read.arff('http://www.cs.usfca.edu/~pfrancislyon/uci-diabetes.arff')
View(diabetes)
t.test(preg ~ class, diabetes)
t.test(plas ~ class, diabetes)
t.test(pres ~ class, diabetes)
t.test(skin ~ class, diabetes)
t.test(insu ~ class, diabetes)
t.test(mass ~ class, diabetes)
t.test(pedi ~ class, diabetes)
t.test(age ~ class, diabetes)
# 3. Plot the density distributions of each variable with fill color determined by diabetes status.
g <- ggplot(melt(diabetes), aes(x = value))
g + facet_wrap(~variable, scales = 'free') +
geom_density(aes(fill = class), alpha = 0.4)
# 4. Check the variables for correlations.
cor(diabetes[-9], use = 'complete')
corrplot(cor(diabetes[-9], use = 'complete'), method='number')
# 5. Fit a regression model to predict diabetes as a function of the other variables.
#    For the significant coefficients, state what one unit increase in each predictor variable
#    will result in for diabetes. In your report discuss the summary.
fit1 <- glm(class ~ preg + plas + pres + skin + insu + mass + pedi + age, diabetes,
family = binomial())
summary(fit1)
# The intercept and plas - plasma glucose concentration are the most significant coificent values
# mass, diabetes pedigree function and age are less significant variables. Others are not significant
# For each unit increase in plas, the result will increase 3.62e-02 unit
# Unit increase in mass -> 7.615e-02 unit increase in diabetes result
# Unit increase in pedi -> 1.097e+00 unit increase in diabetes result
# Unit increase in age -> 4.075e-02 unit increase in diabetes result
# 6. Fit a regression model to predict diastolic blood pressure as a function of two to four of
#    the other predictor variables. For the significant coefficients,
#    state what one unit increase in each predictor variable will result in for diastolic
#    blood pressure. In your report discuss the summary.
fit2 <- lm(pres ~ preg + plas + mass + age, diabetes)
summary(fit2)
# Intercept, mass and age are the most significant coefficients, plas is less significant
# and pedi is the least significant one
# Unit increase in:
# mass > 0.43217 increase in pres
# age > 0.29723 increase in pres
# plas > 0.03420 increase in pres
# preg > 0.30643 increase in pres
install.packages("kernlab")
install.packages("ROCR")
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
#Now we split the data into a training set (80%) and a test set (20%):
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c('Positive Train','Positive Test','Negative Train','Negative Test'),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
# load the kernlab package
library(kernlab)
# train the SVM
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel='vanilladot',C=100,scaled=c())
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example, the support vectors
alpha(svp) # alpha vector (possibly scaled)
# indices of support vectors in data matrix
#  after the possible effect of na.omit and subset
alphaindex(svp)
coef(svp) #The corresponding coefficients times the training labels
# Use the built-in function to pretty-plot the classifier
plot(svp,data=xtrain)
#              plot(scale(x), col=y+2, pch=y+2, xlab="", ylab="")
w <- colSums(coef(svp)[[1]] * x[unlist(alphaindex(svp)),])
b <- b(svp)
# Predict labels on test
ypred = predict(svp,xtest)
table(ytest,ypred)
# Compute accuracy
sum(ypred==ytest)/length(ytest)
# Compute at the prediction scores
ypredscore = predict(svp,xtest,type="decision")
#Practical session: Introduction to SVM in R
#Jean-Philippe Vert
#https://escience.rpi.edu/data/DA/svmbasic_notes.pdf
#install.packages("kernlab")
#install.packages("ROCR")
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
#Now we split the data into a training set (80%) and a test set (20%):
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c('Positive Train','Positive Test','Negative Train','Negative Test'),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
# load the kernlab package
library(kernlab)
# train the SVM
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel='vanilladot',C=100,scaled=c())
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example, the support vectors
alpha(svp) # alpha vector (possibly scaled)
# indices of support vectors in data matrix
#  after the possible effect of na.omit and subset
alphaindex(svp)
coef(svp) #The corresponding coefficients times the training labels
# Use the built-in function to pretty-plot the classifier
plot(svp,data=xtrain)
#              plot(scale(x), col=y+2, pch=y+2, xlab="", ylab="")
w <- colSums(coef(svp)[[1]] * x[unlist(alphaindex(svp)),])
b <- b(svp)
# Predict labels on test
ypred = predict(svp,xtest)
table(ytest,ypred)
# Compute accuracy
sum(ypred==ytest)/length(ytest)
# Compute at the prediction scores
ypredscore = predict(svp,xtest,type="decision")
?sample
table(ytest,ypred)
# Compute accuracy
table(ytest,ypred)
library (ggplot2)
library(foreign)
library (stats)
library (ggplot2)
library(foreign)
library (stats)
logisticPseudoR2s <- function(LogModel) {
dev <- LogModel$deviance
nullDev <- LogModel$null.deviance
modelN <-  length(LogModel$fitted.values)
R.l <-  1 -  dev / nullDev
R.cs <- 1- exp ( -(nullDev - dev) / modelN)
R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))
cat("Pseudo R^2 for logistic regression\n")
cat("Hosmer and Lemeshow R^2  ", round(R.l, 3), "\n")
cat("Cox and Snell R^2        ", round(R.cs, 3), "\n")
cat("Nagelkerke R^2           ", round(R.n, 3),    "\n")
}
diab <- read.arff("http://www.cs.usfca.edu/~pfrancislyon/uci-diabetes.arff")
summary(diab)
diab$plas <- ifelse(diab$plas==0,NA, diab$plas)
diab$pres <- ifelse(diab$pres==0,NA, diab$pres)
diab$skin <- ifelse(diab$skin==0,NA, diab$skin)
diab$insu <- ifelse(diab$insu==0,NA, diab$insu)
diab$mass <- ifelse(diab$mass==0,NA, diab$mass)
summary(diab)
diab2 <- diab
diab2$plas[is.na(diab2$plas)] <- mean(diab2$plas,na.rm=T)
diab2$pres[is.na(diab2$pres)] <- mean(diab2$pres,na.rm=T)
diab2$skin[is.na(diab2$skin)] <- mean(diab2$skin,na.rm=T)
diab2$insu[is.na(diab2$insu)] <- mean(diab2$insu,na.rm=T)
diab2$mass[is.na(diab2$mass)] <- mean(diab2$mass,na.rm=T)
summary(diab2)
library("psych")
d2_attrib <- diab2[,1:8]
library (useful)
hc1 <- hclust(dist(diab2[,1:8]))
plot(hc1)
hc2 <- hclust(dist(diab[,1:8]))
plot(hc2)
hc1 <- hclust(dist(diab2[,1:8]), method = "average")
# plot the dendrogram
plot(hc1)
hc2 <- hclust(dist(diab[,1:8]), method = "average")
plot(hc2)
install.packages('shiny')
library(shiny)
runExample('01_hello')
runApp('my_app')
library(shiny)
server <- function(input, output, session) { } #the server
ui <- basicPage("This is a real Shiny app") # the user interface
shinyApp(ui = ui, server = server)
setwd('~/Github/shinyVM')
wd
getwd
getwd()
library(shiny)
runApp('sample-app')
runApp('sample-apps')
system.file("examples", package="shiny")
runExample('01_hello')
getwd()
runApp('sample-apps/hello')
runApp('~/sample-apps/hello')
getwd()
runApp('~/project/sample-apps/hello')
setwd('~/project')
setwd("~/GitHub/shinyVM/project")
runApp('sample-apps/hello')
runApp('sample-apps/hello', display.mode = 'showcase')
shiny::runApp('sample-apps/hello')
shiny::runApp('sample-apps/hello')
shiny::runApp('sample-apps/hello')
shiny::runApp('sample-apps/hello')
getwd()
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/hello')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
selectInput()
?selectInput()
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
[,2]
faithful[,2]
shiny::runApp('sample-apps/myApp')
counties <- readRDS("~/sample-apps/myApp/data/counties.rds")
getwd()
counties <- readRDS("sample-apps/myApp/data/counties.rds")
counties <- readRDS("sample-apps/myApp/data/counties.rds")
View(counties)
head(counties)
install.packages(c("maps", "mapproj"))
# Note: percent map is designed to work with the counties data set
# Note: percent map is designed to work with the counties data set
# It may not work correctly with other data sets if their row order does
# not exactly match the order in which the maps package plots counties
percent_map <- function(var, color, legend.title, min = 0, max = 100) {
# generate vector of fill colors for map
shades <- colorRampPalette(c("white", color))(100)
# constrain gradient to percents that occur between min and max
var <- pmax(var, min)
var <- pmin(var, max)
percents <- as.integer(cut(var, 100,
include.lowest = TRUE, ordered = TRUE))
fills <- shades[percents]
# plot choropleth map
map("county", fill = TRUE, col = fills,
resolution = 0, lty = 0, projection = "polyconic",
myborder = 0, mar = c(0,0,0,0))
# overlay state borders
map("state", col = "white", fill = FALSE, add = TRUE,
lty = 1, lwd = 1, projection = "polyconic",
myborder = 0, mar = c(0,0,0,0))
# add a legend
inc <- (max - min) / 4
legend.text <- c(paste0(min, " % or less"),
paste0(min + inc, " %"),
paste0(min + 2 * inc, " %"),
paste0(min + 3 * inc, " %"),
paste0(max, " % or more"))
legend("bottomleft",
legend = legend.text,
fill = shades[c(1, 25, 50, 75, 100)],
title = legend.title)
}
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
?percent_map
??percent_map
View(percent_map)
View(percent_map)
shiny::runApp('sample-apps/myApp')
?switch
shiny::runApp('sample-apps/myApp')
shiny::runApp('sample-apps/myApp')
